# ğŸ¤– Chapter 14 â€” The AI Tech Stack: What You Should Know

## 1. Why This Chapter Exists

Artificial Intelligence isnâ€™t a single tool â€” itâ€™s an **ecosystem**. When people say they â€œwork in AI,â€ that can mean anything from designing neural networks to building web apps powered by models in the cloud. This chapter gives you a **map of the AI landscape** so you know whatâ€™s out there, what each piece does, and how they all connect.

> ğŸ’¡ Think of it like understanding the layers of a cake â€” from data collection at the bottom to fancy AI apps on top.

---

## 2. The Big Picture

AI systems usually have **five layers**, each with different technologies and roles:

| Layer                 | Purpose                        | Example Technologies                                    |
| --------------------- | ------------------------------ | ------------------------------------------------------- |
| 1ï¸âƒ£ Data Layer        | Collect, clean, and store data | SQL, MongoDB, BigQuery, Snowflake                       |
| 2ï¸âƒ£ Compute Layer     | Provide hardware power         | GPUs (NVIDIA), TPUs (Google), AWS EC2, Azure ML Compute |
| 3ï¸âƒ£ Model Layer       | Train and serve AI models      | PyTorch, TensorFlow, Hugging Face, ONNX                 |
| 4ï¸âƒ£ Deployment Layer  | Expose models as services      | FastAPI, Flask, Docker, Kubernetes                      |
| 5ï¸âƒ£ Application Layer | Connect AI to users            | React, Streamlit, Gradio, LangChain, OpenAI API         |

These layers mirror the **flow of intelligence** â€” from raw data to user-facing smart features.

---

## 3. Data Layer â€” The Foundation

Before you can train any model, you need **clean, well-structured data**. Data engineering is the plumbing behind AI.

### Common Tools

* **Databases:** PostgreSQL, MongoDB, SQLite
* **Data lakes:** AWS S3, Google Cloud Storage, Hadoop HDFS
* **ETL tools:** Airbyte, Apache Beam, dbt, Airflow
* **Data cleaning:** pandas, NumPy, PySpark

> ğŸ§  Analogy: The data layer is like your kitchen pantry â€” if itâ€™s messy, your meals (models) will taste awful.

---

## 4. Compute Layer â€” The Engines

Training AI models is **computationally expensive**. Youâ€™ll often need specialised chips like **GPUs** or **TPUs** for parallel processing.

### Key Platforms

* **Local compute:** NVIDIA CUDA, PyTorch Lightning
* **Cloud platforms:** Google Colab, AWS Sagemaker, Azure ML, Paperspace
* **Distributed compute:** Ray, Dask, Horovod

> âš™ï¸ GPUs accelerate training by processing many operations simultaneously â€” like hiring 10,000 chefs instead of one to chop ingredients.

---

## 5. Model Layer â€” Where AI Happens

This is where the **learning** happens â€” designing, training, and tuning models.

### Core Frameworks

* **PyTorch** â€” popular among researchers, flexible and Pythonic
* **TensorFlow / Keras** â€” industry favourite, production-ready
* **JAX** â€” Googleâ€™s framework for fast and differentiable computation

### Pretrained Model Ecosystem

* **Hugging Face Transformers** â€” access to thousands of pre-trained models (BERT, GPT-2, T5, etc.)
* **OpenAI API** â€” large models like GPT-4 or DALLÂ·E
* **Stability AI, Anthropic, Cohere** â€” alternative foundation model providers

> ğŸ’¬ You donâ€™t always train models from scratch. Often, you *fine-tune* an existing one, saving time and cost.

---

## 6. From LLMs to Agents â€” The New Evolution

AI models are evolving in how they interact and reason. Letâ€™s break down the terms youâ€™ll hear often:

### ğŸ§  Large Language Models (LLMs)

These are **text-based neural networks** trained on massive corpora to understand and generate human-like language.

* Examples: GPT-4, Claude, Gemini, LLaMA, Mistral.
* They respond to prompts but donâ€™t have memory, planning, or tool use unless programmed externally.

> Analogy: An LLM is like a very smart parrot â€” excellent at language, but it only talks when prompted.

### ğŸ¤– AI Agents

An **AI Agent** is an LLM connected to **tools and memory** â€” it can call APIs, browse data, or write files.

* Examples: LangChain agents, AutoGPT, or OpenAIâ€™s Assistants API.
* They can reason over steps ("first get data, then analyse it") and act autonomously within boundaries.

> Analogy: An AI agent is like a virtual assistant who can *use* a computer â€” not just talk about it.

### ğŸ§© Agentic Systems

This term refers to **multi-agent ecosystems** where several agents collaborate, self-improve, and delegate tasks.

* Examples: swarm intelligence, role-based AI workflows, or systems like **CrewAI**.
* They often include memory persistence, long-term goals, and even emotional modelling.

> Analogy: Agentic AI is like a team of specialised virtual employees â€” researchers, coders, and managers â€” all working together.

---

### 7. Orchestration Frameworks and Libraries

These frameworks make it easier to build, connect, and manage agents or pipelines.

| Library | Focus | Description |
|----------|--------|-------------|
| **LangGraph** | Multi-agent orchestration | Build graph-based workflows where agents pass messages to one another. |
| **LlamaIndex** | Data connectivity library | Connects LLMs to external data sources (like PDFs, databases, or APIs) for retrieval-augmented generation (RAG). It helps the model *find* facts rather than hallucinate. |
| **LangChain** | Agent and tool orchestration library | Provides modular building blocks to let LLMs call functions, manage context, and chain reasoning steps. Itâ€™s the backbone for many AI assistants and chatbots. |
| **Semantic Kernel** | Microsoft orchestration library | Integrates planning, memory, and functions into cohesive AI workflows, especially within the .NET and Python ecosystems. |
| **MCP (Model Context Protocol)** | Open interoperability protocol | Unlike the above libraries, MCP isnâ€™t a coding toolkit â€” itâ€™s a *standard communication protocol* that lets AI models, tools, and data sources communicate securely and contextually across platforms. |
| **smolagents** | Lightweight agent framework/library | A minimal-abstraction Python library from Hugging Face that enables creating agents which write Python code or call tools, support for any LLM, tool-agnostic and model-agnostic. |


---

## 8. Platforms â€” Where Models Live and Run

You donâ€™t always need to host models yourself. Many platforms provide **ready-to-run AI environments**.

| Platform                 | Description                                                                         |
| ------------------------ | ----------------------------------------------------------------------------------- |
| **Ollama**               | Run open-source LLMs locally (e.g. LLaMA, Mistral) with easy model management       |
| **AWS Bedrock**          | Unified API to access models from Anthropic, Stability AI, Cohere, and Amazon Titan |
| **Azure OpenAI Service** | Enterprise integration of GPT, DALLÂ·E, and Codex                                    |
| **Google Vertex AI**     | End-to-end ML platform with model hosting, tuning, and pipelines                    |
| **Hugging Face Hub**     | Cloud for model storage, inference APIs, and community collaboration                |

> â˜ï¸ These platforms act like *AI supermarkets* â€” you can browse, test, and deploy models without deep infrastructure setup.

---

## 9. AI Tools and Products You Should Know

Modern productivity tools are increasingly AI-driven. Here are some popular categories:

### ğŸ§‘â€ğŸ’» Coding & Developer Tools

* **GitHub Copilot** â€” code autocomplete powered by OpenAI Codex.
* **Replit Ghostwriter** â€” integrated coding assistant.
* **Rovo Chat / Rovo Code** â€” conversational code debugging and workflow automation.
* **Tabnine, Cody, Windsurf** â€” AI coding copilots trained for IDEs.

### ğŸ—‚ï¸ Productivity & Communication

* **ChatGPT / Claude / Gemini** â€” general-purpose LLM assistants.
* **Notion AI** â€” document summarisation and idea generation.
* **Jasper / Copy.ai** â€” AI marketing and content generation.

### ğŸ§  Data & Research

* **Perplexity AI** â€” research assistant with source citations.
* **Elicit** â€” AI for literature review and research synthesis.
* **Kaggle Models & Colab** â€” platforms for quick model prototyping.

> ğŸŒ Tip: Keep an eye on integrations â€” the future of work will blend AI assistants directly into browsers, code editors, and dashboards.

---

## 10. Supporting Tools & DevOps for AI

AI development needs **infrastructure discipline** too â€” version control, testing, and monitoring.

* **Versioning:** Git, DVC (Data Version Control)
* **Experiment tracking:** Weights & Biases, MLflow
* **Continuous integration:** GitHub Actions, Jenkins
* **Monitoring:** Prometheus, Grafana, Evidently AI

> ğŸ§© MLOps = DevOps + ML lifecycle management.

---
### ğŸ§‘â€ğŸ’» Coding & Developer Tools

GitHub Copilot â€” code autocomplete powered by OpenAI Codex.

Replit Ghostwriter â€” integrated coding assistant.

Rovo Chat / Rovo Code â€” conversational code debugging and workflow automation.

Cursor â€” an AI-powered code editor based on VS Code, designed for pair-programming with LLMs. It provides intelligent refactoring, context-aware suggestions, and inline chat to assist developers in real time.

Tabnine, Cody, Windsurf â€” AI coding copilots trained for IDEs.

CodeRabbit â€” an AI code review assistant that comments on pull requests, detects issues, and suggests improvements automatically. It integrates with GitHub and GitLab to streamline the review process.

...

### ğŸ§  Data & Research

Perplexity AI â€” research assistant with source citations.

Elicit â€” AI for literature review and research synthesis.

Kaggle Models & Colab â€” platforms for quick model prototyping.

...

### ğŸ—£ï¸ Agent Platforms & AI Builders

Coze (ByteDance) â€” a low-code platform for building and deploying AI agents or chatbots. Supports workflow automation, plugin integration, and deployment across apps and social platforms. Ideal for non-developers who want to create intelligent assistants.

Codex (OpenAI) â€” a specialised large language model trained on source code to assist in software development. Powers tools like GitHub Copilot and can generate, debug, or explain code in multiple languages.

ğŸŒ Tip: Keep an eye on integrations â€” the future of work will blend AI assistants directly into browsers, code editors, and dashboards.

---

### ğŸ§© Summary

* The **AI stack** is a multi-layer ecosystem: Data â†’ Compute â†’ Model â†’ Deployment â†’ Application â†’ Agents.
* Learn progressively â€” start with Python, pandas, and scikit-learn before jumping into LLM orchestration tools.
* Understand the differences between **LLMs**, **AI agents**, and **Agentic systems**.
* Explore frameworks like **LangGraph**, **LlamaIndex**, and **Semantic Kernel**, and platforms like **Ollama** and **AWS Bedrock**.
* Keep experimenting with new AI tools â€” theyâ€™re evolving faster than ever, and fluency with them is your real advantage.
